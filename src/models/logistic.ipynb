{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0ea759c956b1e7f4bf29d5b5b8de3c58cad57223f02d845498a05f680d38f7979",
   "display_name": "Python 3.9.4 64-bit ('nlp': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ea759c956b1e7f4bf29d5b5b8de3c58cad57223f02d845498a05f680d38f7979"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Setup Environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "# Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/vrpo/anaconda3/envs/nlp/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (0,4,10,14,26,32) have mixed types.Specify dtype option on import or set low_memory=False.\n  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1384476192372187142  TRX/USD | $TRXUSD | $TRX $USD\\n\\nDownward tren...   \n",
       "1  1384475936352083974      How did I end up holding a large bag of $TRX?   \n",
       "2  1384475862259621888  Join https://t.co/aZFpfr3XuN ðŸ“ˆNew Binance and ...   \n",
       "3  1384475836779270150  Well I hope you guys have enjoyed this micro b...   \n",
       "4  1384475760342310912  Join https://t.co/aZFpfr3XuN ðŸ“ˆNew Binance and ...   \n",
       "5  1384475622009819137  $XVG - $BTC - Aslong as we can remain above 56...   \n",
       "6  1384475366354395140  Time is running out folks to get in early into...   \n",
       "7  1384475140960960519  $VTC Ready! Send it! Vertcoin to the moon.ðŸš€ðŸš€ðŸš€\\...   \n",
       "8  1384475058488365058  $BTC: $9,476.17 (789.68%)\\n$ETH: $15,316.66 (1...   \n",
       "9  1384474949667069954  The @FUSIONProtocol has got it all: \\ninfrastr...   \n",
       "\n",
       "                source           created_at            author_id token  \\\n",
       "0  Prime Trading Ideas  2021-04-20 11:57:01  1053454356878479361   TRX   \n",
       "1  Twitter for Android  2021-04-20 11:56:00  1258702534958514176   TRX   \n",
       "2   Twitter for iPhone  2021-04-20 11:55:42           3366379990   TRX   \n",
       "3  Twitter for Android  2021-04-20 11:55:36  1354900493932376075   TRX   \n",
       "4   Twitter for iPhone  2021-04-20 11:55:18           3366379990   TRX   \n",
       "5  Twitter for Android  2021-04-20 11:54:45  1383953342645211142   TRX   \n",
       "6  Twitter for Android  2021-04-20 11:53:44  1383953342645211142   TRX   \n",
       "7      Twitter Web App  2021-04-20 11:52:50            207945833   TRX   \n",
       "8       Pipedream, Inc  2021-04-20 11:52:30  1328583929704681472   TRX   \n",
       "9  Twitter for Android  2021-04-20 11:52:04  1383953342645211142   TRX   \n",
       "\n",
       "                          name  \\\n",
       "0                 Prime Trader   \n",
       "1                    Â¥Â¥ CRÂ¥PTO   \n",
       "2                 Crypto Cadet   \n",
       "3                Titanic Tatum   \n",
       "4                 Crypto Cadet   \n",
       "5                      junayed   \n",
       "6                      junayed   \n",
       "7            VertcoinBeastMode   \n",
       "8  $1200 Stimulus is Now Worth   \n",
       "9                      junayed   \n",
       "\n",
       "                                         description         username  \\\n",
       "0  Follow me for real-time charts and technical a...       IdeasPrime   \n",
       "1  I need an account to keep track of the cesspit...        yy_crypto   \n",
       "2                                                NaN    CryptoCadet00   \n",
       "3  crypto enthusiast\\n\\nSpot trader, long term in...     TitanicTatum   \n",
       "4                                                NaN    CryptoCadet00   \n",
       "5                            https://t.co/uei3yafdUK  junayed61732943   \n",
       "6                            https://t.co/uei3yafdUK  junayed61732943   \n",
       "7                                                NaN           Tr_92_   \n",
       "8  Tracking what a US $1200 stimulus check would ...  crypto_stimulus   \n",
       "9                            https://t.co/uei3yafdUK  junayed61732943   \n",
       "\n",
       "                       url  ... STEP3_clean_len STEP4_clean_len  \\\n",
       "0  https://t.co/AssmJaMC30  ...            85.0              70   \n",
       "1  https://t.co/dHOdtKnSG4  ...            45.0              41   \n",
       "2                      NaN  ...           253.0             131   \n",
       "3                      NaN  ...           133.0             115   \n",
       "4                      NaN  ...           252.0             130   \n",
       "5                      NaN  ...           206.0             125   \n",
       "6                      NaN  ...           281.0             157   \n",
       "7                      NaN  ...            84.0              49   \n",
       "8  https://t.co/weGNmUdlm4  ...           269.0             235   \n",
       "9                      NaN  ...           278.0             139   \n",
       "\n",
       "  STEP5_clean_len STEP6_clean_len STEP7_clean_len  STEP8_clean_len  \\\n",
       "0            70.0            70.0            70.0             66.0   \n",
       "1            41.0            41.0            41.0             40.0   \n",
       "2           131.0           131.0           131.0            126.0   \n",
       "3           115.0           115.0           115.0            113.0   \n",
       "4           130.0           130.0           130.0            125.0   \n",
       "5           125.0           125.0           125.0            118.0   \n",
       "6           141.0           131.0           131.0            128.0   \n",
       "7            49.0            49.0            49.0             46.0   \n",
       "8           201.0           201.0           201.0            131.0   \n",
       "9           130.0           115.0           115.0            110.0   \n",
       "\n",
       "   STEP9_clean_len  STEP10_clean_len  STEP11_clean_len  \\\n",
       "0             65.0                65              56.0   \n",
       "1             40.0                40              35.0   \n",
       "2            115.0               149             115.0   \n",
       "3            113.0               113             103.0   \n",
       "4            114.0               148             115.0   \n",
       "5            108.0               108              76.0   \n",
       "6            126.0               126              86.0   \n",
       "7             46.0                67              58.0   \n",
       "8             29.0                29               0.0   \n",
       "9            110.0               110              69.0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  trxusd downward trend long or short it with bt...  \n",
       "1                how do I end up hold a large bag of  \n",
       "2  join :chart_increasing:new binance and kucoin ...  \n",
       "3  well I hope you guy have enjoy this micro bear...  \n",
       "4  join :chart_increasing:new binance and kucoin ...  \n",
       "5  aslong as we can remain above on the r im bull...  \n",
       "6  time be run out folk to get in early into of t...  \n",
       "7  ready send it vertcoin to the moon:rocket::roc...  \n",
       "8                                                NaN  \n",
       "9  the have get it all infrastructure dexbridge b...  \n",
       "\n",
       "[10 rows x 35 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>source</th>\n      <th>created_at</th>\n      <th>author_id</th>\n      <th>token</th>\n      <th>name</th>\n      <th>description</th>\n      <th>username</th>\n      <th>url</th>\n      <th>...</th>\n      <th>STEP3_clean_len</th>\n      <th>STEP4_clean_len</th>\n      <th>STEP5_clean_len</th>\n      <th>STEP6_clean_len</th>\n      <th>STEP7_clean_len</th>\n      <th>STEP8_clean_len</th>\n      <th>STEP9_clean_len</th>\n      <th>STEP10_clean_len</th>\n      <th>STEP11_clean_len</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1384476192372187142</td>\n      <td>TRX/USD | $TRXUSD | $TRX $USD\\n\\nDownward tren...</td>\n      <td>Prime Trading Ideas</td>\n      <td>2021-04-20 11:57:01</td>\n      <td>1053454356878479361</td>\n      <td>TRX</td>\n      <td>Prime Trader</td>\n      <td>Follow me for real-time charts and technical a...</td>\n      <td>IdeasPrime</td>\n      <td>https://t.co/AssmJaMC30</td>\n      <td>...</td>\n      <td>85.0</td>\n      <td>70</td>\n      <td>70.0</td>\n      <td>70.0</td>\n      <td>70.0</td>\n      <td>66.0</td>\n      <td>65.0</td>\n      <td>65</td>\n      <td>56.0</td>\n      <td>trxusd downward trend long or short it with bt...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1384475936352083974</td>\n      <td>How did I end up holding a large bag of $TRX?</td>\n      <td>Twitter for Android</td>\n      <td>2021-04-20 11:56:00</td>\n      <td>1258702534958514176</td>\n      <td>TRX</td>\n      <td>Â¥Â¥ CRÂ¥PTO</td>\n      <td>I need an account to keep track of the cesspit...</td>\n      <td>yy_crypto</td>\n      <td>https://t.co/dHOdtKnSG4</td>\n      <td>...</td>\n      <td>45.0</td>\n      <td>41</td>\n      <td>41.0</td>\n      <td>41.0</td>\n      <td>41.0</td>\n      <td>40.0</td>\n      <td>40.0</td>\n      <td>40</td>\n      <td>35.0</td>\n      <td>how do I end up hold a large bag of</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1384475862259621888</td>\n      <td>Join https://t.co/aZFpfr3XuN ðŸ“ˆNew Binance and ...</td>\n      <td>Twitter for iPhone</td>\n      <td>2021-04-20 11:55:42</td>\n      <td>3366379990</td>\n      <td>TRX</td>\n      <td>Crypto Cadet</td>\n      <td>NaN</td>\n      <td>CryptoCadet00</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>253.0</td>\n      <td>131</td>\n      <td>131.0</td>\n      <td>131.0</td>\n      <td>131.0</td>\n      <td>126.0</td>\n      <td>115.0</td>\n      <td>149</td>\n      <td>115.0</td>\n      <td>join :chart_increasing:new binance and kucoin ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1384475836779270150</td>\n      <td>Well I hope you guys have enjoyed this micro b...</td>\n      <td>Twitter for Android</td>\n      <td>2021-04-20 11:55:36</td>\n      <td>1354900493932376075</td>\n      <td>TRX</td>\n      <td>Titanic Tatum</td>\n      <td>crypto enthusiast\\n\\nSpot trader, long term in...</td>\n      <td>TitanicTatum</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>133.0</td>\n      <td>115</td>\n      <td>115.0</td>\n      <td>115.0</td>\n      <td>115.0</td>\n      <td>113.0</td>\n      <td>113.0</td>\n      <td>113</td>\n      <td>103.0</td>\n      <td>well I hope you guy have enjoy this micro bear...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1384475760342310912</td>\n      <td>Join https://t.co/aZFpfr3XuN ðŸ“ˆNew Binance and ...</td>\n      <td>Twitter for iPhone</td>\n      <td>2021-04-20 11:55:18</td>\n      <td>3366379990</td>\n      <td>TRX</td>\n      <td>Crypto Cadet</td>\n      <td>NaN</td>\n      <td>CryptoCadet00</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>252.0</td>\n      <td>130</td>\n      <td>130.0</td>\n      <td>130.0</td>\n      <td>130.0</td>\n      <td>125.0</td>\n      <td>114.0</td>\n      <td>148</td>\n      <td>115.0</td>\n      <td>join :chart_increasing:new binance and kucoin ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1384475622009819137</td>\n      <td>$XVG - $BTC - Aslong as we can remain above 56...</td>\n      <td>Twitter for Android</td>\n      <td>2021-04-20 11:54:45</td>\n      <td>1383953342645211142</td>\n      <td>TRX</td>\n      <td>junayed</td>\n      <td>https://t.co/uei3yafdUK</td>\n      <td>junayed61732943</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>206.0</td>\n      <td>125</td>\n      <td>125.0</td>\n      <td>125.0</td>\n      <td>125.0</td>\n      <td>118.0</td>\n      <td>108.0</td>\n      <td>108</td>\n      <td>76.0</td>\n      <td>aslong as we can remain above on the r im bull...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1384475366354395140</td>\n      <td>Time is running out folks to get in early into...</td>\n      <td>Twitter for Android</td>\n      <td>2021-04-20 11:53:44</td>\n      <td>1383953342645211142</td>\n      <td>TRX</td>\n      <td>junayed</td>\n      <td>https://t.co/uei3yafdUK</td>\n      <td>junayed61732943</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>281.0</td>\n      <td>157</td>\n      <td>141.0</td>\n      <td>131.0</td>\n      <td>131.0</td>\n      <td>128.0</td>\n      <td>126.0</td>\n      <td>126</td>\n      <td>86.0</td>\n      <td>time be run out folk to get in early into of t...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1384475140960960519</td>\n      <td>$VTC Ready! Send it! Vertcoin to the moon.ðŸš€ðŸš€ðŸš€\\...</td>\n      <td>Twitter Web App</td>\n      <td>2021-04-20 11:52:50</td>\n      <td>207945833</td>\n      <td>TRX</td>\n      <td>VertcoinBeastMode</td>\n      <td>NaN</td>\n      <td>Tr_92_</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>84.0</td>\n      <td>49</td>\n      <td>49.0</td>\n      <td>49.0</td>\n      <td>49.0</td>\n      <td>46.0</td>\n      <td>46.0</td>\n      <td>67</td>\n      <td>58.0</td>\n      <td>ready send it vertcoin to the moon:rocket::roc...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1384475058488365058</td>\n      <td>$BTC: $9,476.17 (789.68%)\\n$ETH: $15,316.66 (1...</td>\n      <td>Pipedream, Inc</td>\n      <td>2021-04-20 11:52:30</td>\n      <td>1328583929704681472</td>\n      <td>TRX</td>\n      <td>$1200 Stimulus is Now Worth</td>\n      <td>Tracking what a US $1200 stimulus check would ...</td>\n      <td>crypto_stimulus</td>\n      <td>https://t.co/weGNmUdlm4</td>\n      <td>...</td>\n      <td>269.0</td>\n      <td>235</td>\n      <td>201.0</td>\n      <td>201.0</td>\n      <td>201.0</td>\n      <td>131.0</td>\n      <td>29.0</td>\n      <td>29</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1384474949667069954</td>\n      <td>The @FUSIONProtocol has got it all: \\ninfrastr...</td>\n      <td>Twitter for Android</td>\n      <td>2021-04-20 11:52:04</td>\n      <td>1383953342645211142</td>\n      <td>TRX</td>\n      <td>junayed</td>\n      <td>https://t.co/uei3yafdUK</td>\n      <td>junayed61732943</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>278.0</td>\n      <td>139</td>\n      <td>130.0</td>\n      <td>115.0</td>\n      <td>115.0</td>\n      <td>110.0</td>\n      <td>110.0</td>\n      <td>110</td>\n      <td>69.0</td>\n      <td>the have get it all infrastructure dexbridge b...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows Ã— 35 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "dtf_pumps = pd.read_csv(\"~/NLP/Project/data/models/tweets.csv\"); \n",
    "dtf_pumps[:10]"
   ]
  },
  {
   "source": [
    "# Train Sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(117065, 35)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "x_train = dtf_pumps[dtf_pumps.train == 1]; x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(157827, 35)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "x_test = dtf_pumps[dtf_pumps.train == 0]; x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(117065,)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "y_train = dtf_pumps[dtf_pumps.train == 1].target; y_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(157827,)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "y_test = dtf_pumps[dtf_pumps.train == 0].target; y_test.shape "
   ]
  },
  {
   "source": [
    "# Logistic Regression: Training\n",
    "## Vectorization: (Count_Vectorizer, TFIDF_Vectorizer)\n",
    "## PARAMS: (ngram(1, 3), nchar(2, 5))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = TfidfVectorizer(ngram_range  = (1, 3),\n",
    "                             stop_words   = 'english',\n",
    "                             analyzer     = 'word'); \n",
    "X_word_tfidf = Vectorizer.fit_transform(x_train.clean_text.values.astype('U')); store_word_tfidf = []; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = TfidfVectorizer(ngram_range  = (2, 5),\n",
    "                             stop_words   = 'english',\n",
    "                             analyzer     = 'char'); \n",
    "X_char_tfidf = Vectorizer.fit_transform(x_train.clean_text.values.astype('U')); store_char_tfidf = []; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer(ngram_range  = (1, 3),\n",
    "                             stop_words   = 'english',\n",
    "                             analyzer     = 'word'); \n",
    "X_word_count = Vectorizer.fit_transform(x_train.clean_text.values.astype('U')); store_word_count = []; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer(ngram_range  = (2, 5),\n",
    "                             stop_words   = 'english',\n",
    "                             analyzer     = 'char'); \n",
    "X_char_count = Vectorizer.fit_transform(x_train.clean_text.values.astype('U')); store_char_count = []; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WORD_TFIDF: 5-CV on Train: 0.6714474864391577\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6713706060735489\n",
      "WORD_COUNT: 5-CV on Train: 0.6620509973091872\n",
      "CHAR_COUNT: 5-CV on Train: 0.6702088583265707\n",
      "WORD_TFIDF: 5-CV on Train: 0.6506983299876137\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6599666851749029\n",
      "WORD_COUNT: 5-CV on Train: 0.6533293469440055\n",
      "CHAR_COUNT: 5-CV on Train: 0.6476401998889505\n",
      "WORD_TFIDF: 5-CV on Train: 0.6492205185153547\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6466407551360355\n",
      "WORD_COUNT: 5-CV on Train: 0.6516721479519925\n",
      "CHAR_COUNT: 5-CV on Train: 0.6414726861145518\n",
      "WORD_TFIDF: 5-CV on Train: 0.649485328663563\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6467090932387991\n",
      "WORD_COUNT: 5-CV on Train: 0.6517575705804466\n",
      "CHAR_COUNT: 5-CV on Train: 0.6358347926365695\n",
      "WORD_TFIDF: 5-CV on Train: 0.6483919190193483\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6462478110451458\n",
      "WORD_COUNT: 5-CV on Train: 0.6438816042369623\n",
      "CHAR_COUNT: 5-CV on Train: 0.608866868833554\n",
      "WORD_TFIDF: 5-CV on Train: 0.6478793832486225\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6459573741084014\n",
      "WORD_COUNT: 5-CV on Train: 0.6432580190492462\n",
      "CHAR_COUNT: 5-CV on Train: 0.6089693759876991\n",
      "WORD_TFIDF: 5-CV on Train: 0.6479648058770768\n",
      "CHAR_TFIDF: 5-CV on Train: 0.645889036005638\n",
      "WORD_COUNT: 5-CV on Train: 0.641259129543416\n",
      "CHAR_COUNT: 5-CV on Train: 0.6237047793960621\n",
      "WORD_TFIDF: 5-CV on Train: 0.6479989749284586\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6443258019049246\n",
      "WORD_COUNT: 5-CV on Train: 0.642455046341776\n",
      "CHAR_COUNT: 5-CV on Train: 0.6260368171528639\n",
      "WORD_TFIDF: 5-CV on Train: 0.647187459958143\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6442916328535429\n",
      "WORD_COUNT: 5-CV on Train: 0.6432580190492462\n",
      "CHAR_COUNT: 5-CV on Train: 0.6026481014820826\n",
      "WORD_TFIDF: 5-CV on Train: 0.6461538461538462\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6441891256993978\n",
      "WORD_COUNT: 5-CV on Train: 0.6430615470038012\n",
      "CHAR_COUNT: 5-CV on Train: 0.6032460598812625\n",
      "WORD_TFIDF: 5-CV on Train: 0.6462050997309187\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6441122453337889\n",
      "WORD_COUNT: 5-CV on Train: 0.6402255157391193\n",
      "CHAR_COUNT: 5-CV on Train: 0.6012813394268142\n",
      "WORD_TFIDF: 5-CV on Train: 0.6456669371716568\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6440609917567164\n",
      "WORD_COUNT: 5-CV on Train: 0.6403365651561097\n",
      "CHAR_COUNT: 5-CV on Train: 0.5976508777175074\n",
      "WORD_TFIDF: 5-CV on Train: 0.645214197240849\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6438388929227352\n",
      "WORD_COUNT: 5-CV on Train: 0.6417033272113782\n",
      "CHAR_COUNT: 5-CV on Train: 0.5969931234784094\n",
      "WORD_TFIDF: 5-CV on Train: 0.6460427967368556\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6408405586639901\n",
      "WORD_COUNT: 5-CV on Train: 0.643095716055183\n",
      "CHAR_COUNT: 5-CV on Train: 0.5971297996839362\n",
      "WORD_TFIDF: 5-CV on Train: 0.6456156835945842\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6409345235552898\n",
      "WORD_COUNT: 5-CV on Train: 0.6425660957587664\n",
      "CHAR_COUNT: 5-CV on Train: 0.5963951650792294\n",
      "WORD_TFIDF: 5-CV on Train: 0.6441122453337889\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6402853115790373\n",
      "WORD_COUNT: 5-CV on Train: 0.6415495664801606\n",
      "CHAR_COUNT: 5-CV on Train: 0.5993337034980566\n",
      "WORD_TFIDF: 5-CV on Train: 0.6434801178832272\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6398496561739204\n",
      "WORD_COUNT: 5-CV on Train: 0.6415153974287788\n",
      "CHAR_COUNT: 5-CV on Train: 0.5972323068380814\n",
      "WORD_TFIDF: 5-CV on Train: 0.643556998248836\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6393285781403494\n",
      "WORD_COUNT: 5-CV on Train: 0.6417802075769872\n",
      "CHAR_COUNT: 5-CV on Train: 0.5985392730534319\n",
      "WORD_TFIDF: 5-CV on Train: 0.6425917225473029\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6389954298893776\n",
      "WORD_COUNT: 5-CV on Train: 0.640421987784564\n",
      "CHAR_COUNT: 5-CV on Train: 0.5987442873617221\n",
      "WORD_TFIDF: 5-CV on Train: 0.6424892153931576\n",
      "CHAR_TFIDF: 5-CV on Train: 0.6387391620040149\n",
      "WORD_COUNT: 5-CV on Train: 0.6400888395335925\n",
      "CHAR_COUNT: 5-CV on Train: 0.5975142015119805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "for i in range(100, 10000, 500):\n",
    "\n",
    "    selector = SelectKBest(chi2, k = i).fit(X_word_tfidf, y_train); X_sel = selector.transform(X_word_tfidf); \n",
    "    model_fit = LogisticRegression(n_jobs = -1); \n",
    "    cv_reg = cross_val_score(model_fit, X_sel, y = y_train, cv = 5, n_jobs = -1, scoring = \"f1_micro\"); \n",
    "    store_word_tfidf.append(cv_reg.mean()); \n",
    "    print(\"WORD_TFIDF: 5-CV on Train: {}\".format(cv_reg.mean()))\n",
    "\n",
    "    selector = SelectKBest(chi2, k = i).fit(X_char_tfidf, y_train); X_sel = selector.transform(X_char_tfidf); \n",
    "    model_fit = LogisticRegression(n_jobs = -1); \n",
    "    cv_reg = cross_val_score(model_fit, X_sel, y = y_train, cv = 5, n_jobs = -1, scoring = \"f1_micro\"); \n",
    "    store_char_tfidf.append(cv_reg.mean()); \n",
    "    print(\"CHAR_TFIDF: 5-CV on Train: {}\".format(cv_reg.mean()))\n",
    "\n",
    "    selector = SelectKBest(chi2, k = i).fit(X_word_count, y_train); X_sel = selector.transform(X_word_count); \n",
    "    model_fit = LogisticRegression(n_jobs = -1); \n",
    "    cv_reg = cross_val_score(model_fit, X_sel, y = y_train, cv = 5, n_jobs = -1, scoring = \"f1_micro\"); \n",
    "    store_word_count.append(cv_reg.mean()); \n",
    "    print(\"WORD_COUNT: 5-CV on Train: {}\".format(cv_reg.mean()))\n",
    "\n",
    "    selector = SelectKBest(chi2, k = i).fit(X_char_count, y_train); X_sel = selector.transform(X_char_count); \n",
    "    model_fit = LogisticRegression(n_jobs = -1); \n",
    "    cv_reg = cross_val_score(model_fit, X_sel, y = y_train, cv = 5, n_jobs = -1, scoring = \"f1_micro\"); \n",
    "    store_char_count.append(cv_reg.mean()); \n",
    "    print(\"CHAR_COUNT: 5-CV on Train: {}\".format(cv_reg.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   store_word_tfidf  store_char_tfidf  store_word_count  store_char_count\n",
       "0          0.671447          0.671371          0.662051          0.670209\n",
       "1          0.650698          0.659967          0.653329          0.647640\n",
       "2          0.649221          0.646641          0.651672          0.641473\n",
       "3          0.649485          0.646709          0.651758          0.635835\n",
       "4          0.648392          0.646248          0.643882          0.608867\n",
       "5          0.647879          0.645957          0.643258          0.608969\n",
       "6          0.647965          0.645889          0.641259          0.623705\n",
       "7          0.647999          0.644326          0.642455          0.626037\n",
       "8          0.647187          0.644292          0.643258          0.602648\n",
       "9          0.646154          0.644189          0.643062          0.603246"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>store_word_tfidf</th>\n      <th>store_char_tfidf</th>\n      <th>store_word_count</th>\n      <th>store_char_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.671447</td>\n      <td>0.671371</td>\n      <td>0.662051</td>\n      <td>0.670209</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.650698</td>\n      <td>0.659967</td>\n      <td>0.653329</td>\n      <td>0.647640</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.649221</td>\n      <td>0.646641</td>\n      <td>0.651672</td>\n      <td>0.641473</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.649485</td>\n      <td>0.646709</td>\n      <td>0.651758</td>\n      <td>0.635835</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.648392</td>\n      <td>0.646248</td>\n      <td>0.643882</td>\n      <td>0.608867</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.647879</td>\n      <td>0.645957</td>\n      <td>0.643258</td>\n      <td>0.608969</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.647965</td>\n      <td>0.645889</td>\n      <td>0.641259</td>\n      <td>0.623705</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.647999</td>\n      <td>0.644326</td>\n      <td>0.642455</td>\n      <td>0.626037</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.647187</td>\n      <td>0.644292</td>\n      <td>0.643258</td>\n      <td>0.602648</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.646154</td>\n      <td>0.644189</td>\n      <td>0.643062</td>\n      <td>0.603246</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "dtf_results = pd.DataFrame(list(zip(store_word_tfidf, store_char_tfidf, store_word_count,store_char_count)), columns = ['store_word_tfidf', 'store_char_tfidf', 'store_word_count', 'store_char_count'])\n",
    "dtf_results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_results.to_csv(\"~/NLP/Project/data/models/results/logisti_results.csv\", index = False); "
   ]
  },
  {
   "source": [
    "# Logistic Regression: Prediction\n",
    "## Model: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = TfidfVectorizer(ngram_range  = (1, 3),\n",
    "                             stop_words   = 'english',\n",
    "                             analyzer     = 'word'); \n",
    "X_word_tfidf = Vectorizer.fit_transform(x_train.clean_text.values.astype('U')); store_word_tfidf = []; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k = 100).fit(X_word_tfidf, y_train); X_sel = selector.transform(X_word_tfidf); \n",
    "model_fit = LogisticRegression().fit(X_sel, y_train); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = Vectorizer.transform(x_test.clean_text.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel_test = selector.transform(X_test); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = []; \n",
    "for i in model_fit.predict_proba(X_sel_test):\n",
    "    predictions.append(i[1]); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Shape of passed values is (157827, 2), indices imply (157827, 1)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1674\u001b[0m                 blocks = [\n\u001b[0;32m-> 1675\u001b[0;31m                     make_block(\n\u001b[0m\u001b[1;32m   1676\u001b[0m                         \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   2741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2742\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_ndim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    143\u001b[0m                 \u001b[0;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 2, placement implies 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-23ea05c0a5e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~/NLP/Project/data/models/results/predictions.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mblock_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1685\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1687\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (157827, 2), indices imply (157827, 1)"
     ]
    }
   ],
   "source": [
    "predictions = pd.DataFrame(predictions, columns = ['predictions'])\n",
    "predictions.to_csv(\"~/NLP/Project/data/models/results/predictions.csv\", index = False); "
   ]
  }
 ]
}